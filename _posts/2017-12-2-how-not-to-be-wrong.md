---
title: How Not to Be Wrong - The Power of Mathematical Thinking
category: reading
permalink: /:categories/:title/
author: Jordan Ellenberg
layout: bookpost
tags:
- math
- mentalmodels
- thinking
---

>  The armor, said Wald, doesn’t go where the bullet holes are. It goes where the bullet holes aren’t: on the engines. Wald’s insight was simply to ask: where are the missing holes? The ones that would have been all over the engine casing, if the damage had been spread equally all over the plane? Wald was pretty sure he knew. The missing bullet holes were on the missing planes. The reason planes were coming back with fewer hits to the engine is that planes that got hit in the engine weren’t coming back. Whereas the large number of planes returning to base with a thoroughly Swiss-cheesed fuselage is pretty strong evidence that hits to the fuselage can (and therefore should) be tolerated.

>  Here’s an old mathematician’s trick that makes the picture perfectly clear: set some variables to zero. In this case, the variable to tweak is the probability that a plane that takes a hit to the engine manages to stay in the air. Setting that probability to zero means a single shot to the engine is guaranteed to bring the plane down.

>  One thing the American defense establishment has traditionally understood very well is that countries don’t win wars just by being braver than the other side, or freer, or slightly preferred by God. The winners are usually the guys who get 5% fewer of their planes shot down, or use 5% less fuel, or get 5% more nutrition into their infantry at 95% of the cost.

>  Mathematics is the study of things that come out a certain way because there is no other way they could possibly be.

>  To paraphrase Clausewitz: Mathematics is the extension of common sense by other means.

>  The difference between the two pictures is the difference between linearity and nonlinearity, one of the central distinctions in mathematics.

>  Nonlinear thinking means which way you should go depends on where you already are.

>  A basic rule of mathematical life: if the universe hands you a hard problem, try to solve an easier one instead, and hope the simple version is close enough to the original problem that the universe doesn’t object.

>  The great insight of Eudoxus and Archimedes was that it doesn’t matter whether it’s a circle or a polygon with very many very short sides. The two areas will be close enough for any purpose you might have in mind. The area of the little fringe between the circle and the polygon has been “exhausted” by our relentless iteration.

>  The slogan to keep in mind: straight locally, curved globally.

>  Now here’s the conceptual leap. Newton said, look, let’s go all the way. Reduce your field of view until it’s infinitesimal—so small that it’s smaller than any size you can name, but not zero. You’re studying the missile’s arc, not over a very short time interval, but at a single moment. What was almost a line becomes exactly a line. And the slope of this line is what Newton called the fluxion, and what we’d now call the derivative.

>  he would never have said that the circle actually was a polygon with infinitely many infinitely short sides.

>  If 9 times something is 9, that something just has to be 1—doesn’t it?

>  nonstandard analysis. The theory, developed by Abraham Robinson in the mid-twentieth century, finally made sense of the “evanescent increments” that Berkeley found so ridiculous. The price you have to pay (or, from another point of view, the reward you get to reap) is a profusion of novel kinds of numbers; not only infinitely small ones, but infinitely large ones, a huge spray of them in all shapes and sizes.*

>  In the real world, you can never have infinitely many heaps. What’s the numerical value of an infinite sum? It doesn’t have one—until we give it one. That was the great innovation of Augustin-Louis Cauchy, who introduced the notion of limit into calculus in the 1820s.*

>  In the mathematical context, the good choices are the ones that settle unnecessary perplexities without creating new ones.

>  We can’t hold on to both of these desires at once; one must be discarded. In Cauchy’s approach, which has amply proved its worth in the two centuries since he invented it, it’s the uniqueness of the decimal expansion that goes out the window. We’re untroubled by the fact that the English language sometimes uses two different strings of letters (i.e., two words) to refer synonymously to the same thing in the world; in the same way, it’s not so bad that two different strings of digits can refer to the same number.

>  I have some good news. We’re not all going to be overweight in the year 2048. Why? Because not every curve is a line. But every curve, as we just learned from Newton, is pretty close to a line. That’s the idea that drives linear regression, the statistical technique that is to social science as the screwdriver is to home repair.

>  Whenever you want to understand which variables drive which other variables, and in which direction, it’s the first thing you reach for. And it works on any data set at all. That’s a weakness as well as a strength. You can do linear regression without thinking about whether the phenomenon you’re modeling is actually close to linear. But you shouldn’t. I said linear regression was like a screwdriver, and that’s true; but in another sense, it’s more like a table saw. If you use it without paying careful attention to what you’re doing, the results can be gruesome.

>  The danger of overemphasizing algorithms and precise computations is that algorithms and precise computations are easy to assess. If we settle on a vision of mathematics that consists of “getting the answer right” and no more, and test for that, we run the risk of creating students who test very well but know no mathematics at all.

>  An important rule of mathematical hygiene: when you’re field-testing a mathematical method, try computing the same thing several different ways. If you get several different answers, something’s wrong with your method.

>  That something is the cold, strong hand of the Law of Large Numbers. I won’t state that theorem precisely (though it is stunningly handsome!), but you can think of it as saying the following: the more coins you flip, the more and more extravagantly unlikely it is that you’ll get 80% heads. In fact, if you flip enough coins, there’s only the barest chance of getting as many as 51%! Observing a highly unbalanced result in ten flips is unremarkable; getting the same proportional imbalance in a hundred flips would be so startling as to make you wonder whether someone has mucked with your coins.

>  Scoring by raw number of heads gives the Big team an insuperable advantage; but using percentages slants the game just as badly in favor of the Smalls. The smaller the number of coins—what we’d call in statistics the sample size—the greater the variation in the proportion of heads.

>  De Moivre’s insight is that the size of the typical discrepancy* is governed by the square root of the number of coins you toss. Toss a hundred times as many coins as before and the typical discrepancy grows by a factor of 10—at least, in absolute terms. As a proportion of the total number of tosses, the discrepancy shrinks as the number of coins grows, because the square root of the number of coins grows much more slowly than does the number of coins itself.

>  That’s how the Law of Large Numbers works: not by balancing out what’s already happened, but by diluting what’s already happened with new data, until the past is so proportionally negligible that it can safely be forgotten.

>  Here’s a rule of thumb that makes sense to me: if the magnitude of a disaster is so great that it feels right to talk about “survivors,” then it makes sense to measure the death toll as a proportion of total population.

>  Most mathematicians would say that, in the end, the disasters and atrocities of history form what we call a partially ordered set. That’s a fancy way of saying that some pairs of disasters can be meaningfully compared and others cannot.

>  Don’t talk about percentages of numbers when the numbers might be negative.

>  all shared a taste for the strand of Torah study that searches for esoteric texts hidden beneath the stories, genealogies, and admonitions that make up the Torah’s surface. Their tool of choice was the “equidistant letter sequence,” henceforth ELS, a string of text obtained by plucking characters from the Torah at regular intervals.

>  The Baltimore stockbroker con works because, like all good magic tricks, it doesn’t try to fool you outright. That is, it doesn’t try to tell you something false—rather, it tells you something true from which you’re likely to draw incorrect conclusions. It really is improbable that ten stock picks in a row would come out the right way, or that a magician who bet on six horse races would get the winner right every time, or that a mutual fund would beat the market by 10%. The mistake is in being surprised by this encounter with the improbable. The universe is big, and if you’re sufficiently attuned to amazingly improbable occurrences, you’ll find them. Improbable things happen a lot.

>  Aristotle, as usual, was here first: despite lacking any formal notion of probability, he was able to understand that “it is probable that improbable things will happen. Granted this, one might argue that what is improbable is probable.”

>  Once you’ve truly absorbed this fundamental truth, the Baltimore stockbroker has no power over you. That the stockbroker handed you ten straight good stock picks is very unlikely; that he handed somebody such a good run of picks, given ten thousand chances, is not even remotely surprising. In the British statistician R. A. Fisher’s famous formulation, “the ‘one chance in a million’ will undoubtedly occur, with no less and no more than its appropriate frequency, however surprised we may be that it should occur to us.”

>  When you’re trying to draw reliable inferences from improbable events, wiggle room is the enemy.

>  The more chances you give yourself to be surprised, the higher your threshold for surprise had better be.

>  all.

>  Improbability, as described here, is a relative notion, not an absolute one; when we say an outcome is improbable, we are always saying, explicitly or not, that it is improbable under some set of hypotheses we’ve made about the underlying mechanisms of the world.

>  as described here, is a relative notion, not an absolute one; when we say an outcome is improbable, we are always saying, explicitly or not, that it is improbable under some set of hypotheses we’ve made about the underlying mechanisms of the world.

>  Many scientific questions can be boiled down to a simple yes or no: Is something going on, or not? Does a new drug make a dent in the illness it proposes to cure, or does it do nothing? Does a psychological intervention make you happier/ peppier/ sexier or does it do nothing at all? The “does nothing” scenario is called the null hypothesis. That is, the null hypothesis is the hypothesis that the intervention you’re studying has no effect.

>  It’s not enough that the data be consistent with your theory; they have to be inconsistent with the negation of your theory, the dreaded null hypothesis.

>  A standard textbook calls the method “the backbone of psychological research.” It’s the standard by which we separate experiments into successes and failures. Every time you encounter the results of a medical, psychological, or economic research study, you’re very likely reading about something that was vetted by a significance test.

>  Mathematical research articles, sometimes to the surprise of outsiders, are not predominantly composed of numerals and symbols; math is made of words.

>  New things require new vocabulary. There are two ways to go. You can cut new words from fresh cloth, as we do when we speak of cohomology, syzygies, monodromy, and so on; this has the effect of making our work look forbidding and unapproachable. More commonly, we adapt existing words for our own purposes, based on some perceived resemblance between the mathematical object to be described and a thing in the so-called real world.

>  So: significance. In common language it means something like “important” or “meaningful.” But the significance test that scientists use doesn’t measure importance. When we’re testing the effect of a new drug, the null hypothesis is that there is no effect at all; so to reject the null hypothesis is merely to make a judgment that the effect of the drug is not zero. But the effect could still be very small—so small that the drug isn’t effective in any sense that an ordinary non-mathematical Anglophone would call significant.

>  That’s the power of the method, but also its danger. The truth is, the null hypothesis, if we take it literally, is probably just about always false. When you drop a powerful drug into a patient’s bloodstream, it’s hard to believe the intervention has exactly zero effect on the probability that the patient will develop esophageal cancer, or thrombosis, or bad breath.

>  Just because we can detect them doesn’t always mean they matter.

>  But Skinner was wrong; he hadn’t proved that Shakespeare didn’t alliterate. A significance test is an instrument, like a telescope. And some instruments are more powerful than others. If you look at Mars with a research-grade telescope, you’ll see moons; if you look with binoculars, you won’t. But the moons are still there!

>  A statistical study that’s not refined enough to detect a phenomenon of the expected size is called underpowered—the equivalent of looking at the planets with binoculars.

>  This didn’t faze Tversky, who relished a good fight, whatever the outcome. “I’ve been in a thousand arguments over this topic,” he said. “I’ve won them all, and I’ve convinced no one.”

>  A player who made a layup was no more likely to shoot from distance than a player who just missed a layup. Layups are easy and shouldn’t give the player a strong sense of being hot. But a player is much more likely to try a long shot after a three-point basket than after a three-point miss. In other words, the hot hand might “cancel itself out”—players, believing themselves to be hot, get overconfident and take shots they shouldn’t.

>  Assuming the truth of something we quietly believe to be false is a time-honored method of argument that goes all the way back to Aristotle; it is the proof by contradiction, or reductio ad absurdum.

>  The reductio is a kind of mathematical judo, in which we first affirm what we wish eventually to deny, with the plan of throwing it over our shoulder and defeating it by means of its own force. If a hypothesis implies a falsehood,* then the hypothesis itself must be false.

>  Joseph Berkson, the longtime head of the medical statistics division at the Mayo Clinic, who cultivated (and loudly broadcast) a vigorous skepticism about methodology he thought shaky, offered a famous example demonstrating the pitfalls of the method. Suppose you have a group of fifty experimental subjects, who you hypothesize (H) are human beings. You observe (O) that one of them is an albino. Now, albinism is extremely rare, affecting no more than one in twenty thousand people. So given that H is correct, the chance you’d find an albino among your fifty subjects is quite small, less than 1 in 400,* or 0.0025. So the p-value, the probability of observing O given H, is much lower than .05. We are inexorably led to conclude, with a high degree of statistical confidence, that H is incorrect: the subjects in the sample are not human beings.

>  Every positive number can be expressed in just one way as a product of prime numbers.

>  At this point I’m sometimes asked, “Why is the product of no primes 1, and not 0?” Here’s one slightly convoluted explanation: If you take the product of some set of primes, like 2 and 3, but then divide away the very primes you multiplied, you ought to be left with the product of nothing at all; and 6 divided by 6 is 1, not 0. (The sum of no numbers, on the other hand, is indeed 0.)

>  The primes are the atoms of number theory, the basic indivisible entities of which all numbers are made.

>  One of the first theorems ever proved in number theory is that of Euclid, which tells us that the primes are infinite in number; we will never run out, no matter how far along the number line we let our minds range.

>  Among the first N numbers, about N/ log N are prime; this is the Prime Number Theorem, proven at the end of the nineteenth century by the number theorists Jacques Hadamard and Charles-Jean de la Vallée Poussin.

>  The logarithm of a positive number N, called log N, is the number of digits it has.

>  The flogarithm (whence also the logarithm) is a very slowly growing function indeed: the flogarithm of a thousand is 4, the flogarithm of a million, a thousand times greater, is 7, and the flogarithm of a billion is still only 10.*

>  The primes are not random, but it turns out that in many ways they act as if they were.

>  But an old theorem of Dirichlet tells us that remainder 1 shows up about equally as often as remainder 2, just as is the case for random numbers. So as far as “remainder when divided by 3” goes, prime numbers, apart from not being multiples of 3, look random.

>  But what Zhang proved is that there are infinitely many pairs of primes that differ by at most 70 million. In other words, that the gap between one prime and the next is bounded by 70 million infinitely often—thus, the “bounded gaps” conjecture.

>  How wonderfully paradoxical: what helps us break down the final mysteries about prime numbers may be new mathematical ideas that structure the concept of structurelessness itself.

>  Remember the definition of the p-value; this says precisely that if the null hypothesis is true for some particular experiment, then the chance that that experiment will nonetheless return a statistically significant result is only 1 in 20.

>  Scientists call this problem “the winner’s curse,” and it’s one reason that impressive and loudly touted experimental results often melt into disappointing sludge when the experiments are repeated.

>  “p-hacking.” Hacking the p isn’t usually as crude as I’ve made it out to be, and it’s seldom malicious. The p-hackers truly believe in their hypotheses, just as the Bible coders do, and when you’re a believer, it’s easy to come up with reasons that the analysis that gives a publishable p-value is the one you should have done in the first place.

>  to determine the truth from the evidence? Their startling response is to unask the question. For Neyman and Pearson, the purpose of statistics isn’t to tell us what to believe, but to tell us what to do. Statistics is about making decisions, not answering questions.

>  A statistically significant finding gives you a clue, suggesting a promising place to focus your research energy. The significance test is the detective, not the judge.

>  if you ask people to pick a number between 1 and 20, 17 is the most common choice. And if you ask people for a number between 0 and 9, they most frequently pick 7. Numbers ending in 0 and 5, by contrast, are chosen much more rarely than chance would lead you to expect—they just seem less random to people. This leads to an irony. Just as the radio psychic contestants tried to match random sequences of Rs and Bs and produced notably nonrandom results, so people who choose random numbers tend to make choices that visibly deviate from randomness.

>  Just as the prior describes your beliefs before you see the evidence, the posterior describes your beliefs afterward. What we’re doing here is called Bayesian inference, because the passage from prior to posterior rests on an old formula in probability called Bayes’s Theorem.

>  In the Bayesian framework, how much you believe something after you see the evidence depends not just on what the evidence shows, but on how much you believed it to begin with.

>  On the contrary. When Fisher says that “no scientific worker has a fixed level of significance at which from year to year, and in all circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light of his evidence and his ideas,” he is saying exactly that scientific inference can’t, or at least shouldn’t, be carried out purely mechanically; our preexisting ideas and beliefs must always be allowed to play a part.

>  When we say that there’s a 5% chance that RED is true, we are making a statement not about the global distribution of biased roulette wheels (how could we know?) but rather about our own mental state. Five percent is the degree to which we believe that a roulette wheel we encounter is weighted toward the red. This

>  It explains why five reds in a row feels “less random” than RBRRB to us; it’s because the former activates a theory, RED, to which we assign some non-negligible prior probability, and the latter doesn’t.

>  principle of indifference—since there can be no principled way to pretend we don’t know we exist, we just divvy up the prior probability evenly, 50% for GOD and 50% for NO GOD.

>  “It is an old maxim of mine that when you have excluded the impossible, whatever remains, however improbable, must be the truth.”

>  But it doesn’t tell the whole story. What Sherlock Holmes should have said was: “It is an old maxim of mine that when you have excluded the impossible, whatever remains, however improbable, must be the truth, unless the truth is a hypothesis it didn’t occur to you to consider.”

>  The attraction of lotteries is no novelty. The practice dates back to seventeenth-century Genoa, where it seems to have evolved by accident from the electoral system. Every six months, two of the city’s governatori were drawn from the members of the Petty Council.

>  There is not, however, a more certain proposition in mathematics, than that the more tickets you adventure upon, the more likely you are to be a loser. Adventure upon all the tickets in the lottery, and you lose for certain; and the greater the number of your tickets, the nearer you approach to this certainty.

>  ADDITIVITY: The expected value of the sum of two things is the sum of the expected value of the first thing with the expected value of the second thing.

>  When you’re faced with a math problem you don’t know how to do, you’ve got two basic options. You can make the problem easier, or you can make it harder.

>  Other times, your simplification is so simple that it eliminates the interesting features of the problem, as in the old joke about the physicist tasked with optimizing dairy production: he begins, with great confidence, “Consider a spherical cow . . .”

>  The unknown is a stone in the sea, which obstructs our progress. We can try to pack dynamite in the crevices of rock, detonate it, and repeat until the rock breaks apart, as Buffon did with his complicated computations in calculus. Or you can take a more contemplative approach, allowing your level of understanding gradually and gently to rise, until after a time what appeared as an obstacle is overtopped by the calm water, and is gone.

>  Math, like meditation, puts you in direct contact with the universe, which is bigger than you, was here before you, and will be here after you.

>  The more tickets sold, the more revenue comes in. The state doesn’t care who wins. The state just cares how many people play.

>  Their approach was governed by a simple maxim: if gambling is exciting, you’re doing it wrong.

>  Or at least there’s no such currency on paper. But decisions must be made, and economists aspire to tell us how to make them, and so some version of the annoyingness dollar must be constructed. The standard economic story is that human beings, when they’re acting rationally, make decisions that maximize their utility.

>  The condundrum, known as the St. Petersburg paradox, had been devised by Nicolas Bernoulli, Daniel’s cousin, some thirty years before, and many of the probabilists of the time had puzzled over it without coming to any satistfying conclusion.

>  The mistake, Bernoulli said, is to say that a ducat is a ducat is a ducat. A ducat in the hand of a rich man is not worth the same as a ducat in the hand of a peasant, as is plainly visible from the different levels of care with which the two men treat their cash.

>  Twice as many ducats doesn’t translate into twice as many utils; not all curves are lines, and the relation between money and utility is governed by one of those nonlinear curves.

