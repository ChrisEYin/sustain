---
title: Thinking in Systems - A Primer
category: books
permalink: /:categories/:title/
author:  Donella H. Meadows
layout: bookpost
tags:
- systems
- mentalmodels
- thinking
- textbook
- loops
---

>  A system* is an interconnected set of elements that is coherently organized in a way that achieves something. If you look at that definition closely for a minute, you can see that a system must consist of three kinds of things: elements, interconnections, and a function or purpose.

>  A system is more than the sum of its parts. It may exhibit adaptive, dynamic, goal-seeking, self-preserving, and sometimes evolutionary behavior.

>  Elements do not have to be physical things. Intangibles are also elements of a system. In a university, school pride and academic prowess are two intangibles that can be very important elements of the system.

>  Many of the interconnections in systems operate through the flow of information. Information holds systems together and plays a great role in determining how they operate.

>  Purposes are deduced from behavior, not from rhetoric or stated goals.

>  An important function of almost every system is to ensure its own perpetuation.

>  Systems can be nested within systems. Therefore, there can be purposes within purposes.

>  Keeping sub-purposes and overall system purposes in harmony is an essential function of successful systems.

>  If the interconnections change, the system may be greatly altered. It may even become unrecognizable, even though the same players are on the team. Change the rules from those of football to those of basketball, and you’ve got, as they say, a whole new ball game. If you change the interconnections in the tree—say that instead of taking in carbon dioxide and emitting oxygen, it does the reverse—it would no longer be a tree. (It would be an animal.)

>  A change in purpose changes a system profoundly, even if every element and interconnection remains the same.

>  A stock is the foundation of any system. Stocks are the elements of the system that you can see, feel, count, or measure at any given time.

>  A stock, then, is the present memory of the history of changing flows within the system.

>  All models, whether mental models or mathematical models, are simplifications of the real world.

>  As long as the sum of all inflows exceeds the sum of all outflows, the level of the stock will rise. • As long as the sum of all outflows exceeds the sum of all inflows, the level of the stock will fall. • If the sum of all outflows equals the sum of all inflows, the stock level will not change; it will be held in dynamic equilibrium at whatever level it happened to be when the two sets of flows became equal.

>  A stock can be increased by decreasing its outflow rate as well as by increasing its inflow rate.

>  Stocks generally change slowly, even when the flows into or out of them change suddenly. Therefore, stocks act as delays or buffers or shock absorbers in systems.

>  The time lags imposed by stocks allow room to maneuver, to experiment, and to revise policies that aren’t working.

>  Stocks allow inflows and outflows to be decoupled and to be independent and temporarily out of balance with each other.

>  Systems thinkers see the world as a collection of stocks along with the mechanisms for regulating the levels in the stocks by manipulating flows. That means system thinkers see the world as a collection of “feedback processes.”

>  A feedback loop is a closed chain of causal connections from a stock, through a set of decisions or rules or physical laws or actions that are dependent on the level of the stock, and back again through a flow to change the stock.

>  This kind of stabilizing, goal-seeking, regulating loop is called a balancing feedback loop, so I put a B inside the loop in the diagram. Balancing feedback loops are goal-seeking or stability-seeking. Each tries to keep a stock at a given value or within a range of values. A balancing feedback loop opposes whatever direction of change is imposed on the system. If you push a stock too far up, a balancing loop will try to pull it back down. If you shove it too far down, a balancing loop will try to bring it back up.

>  Balancing feedback loops are equilibrating or goal-seeking structures in systems and are both sources of stability and sources of resistance to change.

>  The second kind of feedback loop is amplifying, reinforcing, self-multiplying, snowballing—a vicious or virtuous circle that can cause healthy growth or runaway destruction. It is called a reinforcing feedback loop, and will be noted with an R in the diagrams. It generates more input to a stock the more that is already there (and less input the less that is already there). A reinforcing feedback loop enhances whatever direction of change is imposed on it.

>  Reinforcing feedback loops are self-enhancing, leading to exponential growth or to runaway collapses over time. They are found whenever a stock has the capacity to reinforce or reproduce itself.

>  The time it takes for an exponentially growing stock to double in size, the “doubling time,” equals approximately 70 divided by the growth rate (expressed as a percentage).

>  You’ll stop looking for who’s to blame; instead you’ll start asking, “What’s the system?” The concept of feedback opens up the idea that a system can cause its own behavior.

>  The . . . goal of all theory is to make the . . . basic elements as simple and as few as possible without having to surrender the adequate representation of . . . experience. —Albert Einstein, 1 physicist

>  A Stock with Two Competing Balancing Loops—a Thermostat

>  The information delivered by a feedback loop can only affect future behavior; it can’t deliver the information, and so can’t have an impact fast enough to correct behavior that drove the current feedback. A person in the system who makes a decision based on the feedback can’t change the behavior of the system that drove the current feedback; the decisions he or she makes will affect only future behavior.

>  A stock-maintaining balancing feedback loop must have its goal set appropriately to compensate for draining or inflowing processes that affect that stock. Otherwise, the feedback process will fall short of or exceed the target for the stock.

>  A Stock with One Reinforcing Loop and One Balancing Loop—Population and Industrial Economy

>  Complex behaviors of systems often arise as the relative strengths of feedback loops shift, causing first one loop and then another to dominate behavior.

>  System dynamics models explore possible futures and ask “what if” questions.

>  QUESTIONS FOR TESTING THE VALUE OF A MODEL 1. Are the driving factors likely to unfold this way? 2. If they did, would the system react this way? 3. What is driving the driving factors?

>  Systems with similar feedback structures produce similar dynamic behaviors.

>  One of the central insights of systems theory, as central as the observation that systems largely cause their own behavior, is that systems with similar feedback structures produce similar dynamic behaviors, even if the outward appearance of these systems is completely dissimilar.

>  A delay in a balancing feedback loop makes a system likely to oscillate.

>  Delays are pervasive in systems, and they are strong determinants of behavior. Changing the length of a delay may (or may not, depending on the type of delay and the relative lengths of other delays) make a large change in the behavior of a system.

>  And we are aware that some delays can be powerful policy levers. Lengthening or shortening them can produce major changes in the behavior of systems.

>  In physical, exponentially growing systems, there must be at least one reinforcing loop driving the growth and at least one balancing loop constraining the growth, because no physical system can grow forever in a finite environment.

>  A quantity growing exponentially toward a constraint or limit reaches that limit in a surprisingly short time.

>  The real choice in the management of a nonrenewable resource is whether to get rich very fast or to get less rich but stay that way longer.

>  will just point out that, according to the dynamics of depletion, the larger the stock of initial resources, the more new discoveries, the longer the growth loops elude the control loops, and the higher the capital stock and its extraction rate grow, and the earlier, faster, and farther will be the economic fall on the back side of the production peak.

>  I’ve shown three sets of possible behaviors of this renewable resource system here: • overshoot and adjustment to a sustainable equilibrium, • overshoot beyond that equilibrium followed by oscillation around it, and • overshoot followed by collapse of the resource and the industry dependent on the resource.

>  The trick, as with all the behavioral possibilities of complex systems, is to recognize what structures contain which latent behaviors, and what conditions release those behaviors—and, where possible, to arrange the structures and conditions to reduce the probability of destructive behaviors and to encourage the possibility of beneficial ones.

>  Why do systems work so well? Consider the properties of highly functional systems—machines or human communities or ecosystems—which are familiar to you. Chances are good that you may have observed one of three characteristics: resilience, self-organization, or hierarchy.

>  Resilience is a measure of a system’s ability to survive and persist within a variable environment. The opposite of resilience is brittleness or rigidity.

>  Resilience arises from a rich structure of many feedback loops that can work in different ways to restore a system even after a large perturbation.

>  set of feedback loops that can restore or rebuild feedback loops is resilience at a still higher level—meta-resilience, if you will.

>  This distinction between static stability and resilience is important. Static stability is something you can see; it’s measured by variation in the condition of a system week by week or year by year. Resilience is something that may be very hard to see, unless you exceed its limits, overwhelm and damage the balancing loops, and the system structure breaks down.

>  Because resilience may not be obvious without a whole-system view, people often sacrifice resilience for stability, or for productivity, or for some other more immediately recognizable system property.

>  Loss of resilience can come as a surprise, because the system usually is paying much more attention to its play than to its playing space.

>  Systems need to be managed not only for productivity or stability, they also need to be managed for resilience—the ability to recover from perturbation, the ability to restore or repair themselves.

>  This capacity of a system to make its own structure more complex is called self-organization.

>  Productivity and stability are the usual excuses for turning creative human beings into mechanical adjuncts to production processes.

>  New discoveries, however, suggest that just a few simple organizing principles can lead to wildly diverse self-organizing structures.

>  The result is called a Koch snowflake. (See Figure 46.) Its edge has tremendous length—but it can be contained within a circle. This structure is one simple example of fractal geometry—a realm of mathematics and art populated by elaborate shapes formed by relatively simple rules.

>  Here are some other examples of simple organizing rules that have led to self-organizing systems of great complexity:

>  The world, or at least the parts of it humans think they understand, is organized in subsystems aggregated into larger subsystems, aggregated into still larger subsystems.

>  Complex systems can evolve from simple systems only if there are stable intermediate forms. The resulting complex forms will naturally be hierarchic.

>  In hierarchical systems relationships within each subsystem are denser and stronger than relationships between subsystems.

>  Much can be learned by taking apart systems at different hierarchical levels—cells or organs, for example—and studying them separately. Hence, systems thinkers would say, the reductionist dissection of regular science teaches us a lot. However,

>  Hierarchies evolve from the lowest level up—from the pieces to the whole, from cell to organ to organism, from individual to team, from actual production to management of production. Early farmers decided to come together and form cities for self-protection and for making trade more efficient.

>  The original purpose of a hierarchy is always to help its originating subsystems do their jobs better.

>  When a subsystem’s goals dominate at the expense of the total system’s goals, the resulting behavior is called suboptimization.

>  Just as damaging as suboptimization, of course, is the problem of too much central control. If the brain controlled each cell so tightly that the cell could not perform its self-maintenance functions, the whole organism could die.

>  Hierarchical systems evolve from the bottom up. The purpose of the upper layers of the hierarchy is to serve the purposes of the lower layers.

>  The interactions between what I think I know about dynamic systems and my experience of the real world never fails to be humbling. They keep reminding me of three truths:

>  1. Everything we think we know about the world is a model.

>  2. Our models usually have a strong congruence with the world.

>  3. However, and conversely, our models fall far short of representing the world fully.

>  Our knowledge is amazing; our ignorance even more so.

>  Everything we think we know about the world is a model. Our models do have a strong congruence with the world. Our models fall far short of representing the real world fully.

>  Systems fool us by presenting themselves—or we fool ourselves by seeing the world—as a series of events. The daily news tells of elections, battles, political agreements, disasters, stock market booms or busts.

>  Events are the outputs, moment by moment, from the black box of the system.

>  The behavior of a system is its performance over time—its growth, stagnation, decline, oscillation, randomness, or evolution.

>  When a systems thinker encounters a problem, the first thing he or she does is look for data, time graphs, the history of the system. That’s because long term behavior provides clues to the underlying system structure. And structure is the key to understanding not just what is happening, but why.

>  The structure of a system is its interlocking stocks, flows, and feedback loops.

>  Structure determines what behaviors are latent in the system. A goal-seeking balancing feedback loop approaches or holds a dynamic equilibrium. A reinforcing feedback loop generates exponential growth.

>  System structure is the source of system behavior. System behavior reveals itself as a series of events over time.

>  These behavior-based models are more useful than event-based ones, but they still have fundamental problems. First, they typically overemphasize system flows and underemphasize stocks.

>  Let me use a simple example to explain what I mean. Suppose you knew nothing at all about thermostats, but you had a lot of data about past heat flows into and out of the room. You could find an equation telling you how those flows have varied together in the past, because under ordinary circumstances, being governed by the same stock (temperature of the room), they do vary together. Your equation would hold, however, only until something changes in the system’s structure—someone opens a window or improves the insulation, or tunes the furnace, or forgets to order oil.

>  That’s why behavior-based econometric models are pretty good at predicting the near-term performance of the economy, quite bad at predicting the longer-term performance, and terrible at telling one how to improve the performance of the economy.

>  Nonlinearities are important not only because they confound our expectations about the relationship between action and response. They are even more important because they change the relative strengths of feedback loops. They can flip a system from one mode of behavior to another.

>  Beware of clouds! They are prime sources of system surprises.

>  Which is not to say that every model, mental or computer, has to follow each connection until it includes the whole planet. Clouds are a necessary part of models that describe metaphysical flows.

>  we’re to understand anything, we have to simplify, which means we have to make boundaries.

>  There are no separate systems. The world is a continuum. Where to draw a boundary around a system depends on the purpose of the discussion—the questions we want to ask.

>  Ideally, we would have the mental flexibility to find the appropriate boundary for thinking about each new problem. We are rarely that flexible. We get attached to the boundaries our minds happen to be accustomed

>  It’s a great art to remember that boundaries are of our own making, and that they can and should be reconsidered for each new discussion, problem, or purpose.

>  It was with regard to grain that Justus von Liebig came up with his famous “law of the minimum.”

>  This concept of a limiting factor is simple and widely misunderstood.

>  At any given time, the input that is most important to a system is the one that is most limiting.

>  Insight comes not only from recognizing which factor is limiting, but from seeing that growth itself depletes or enhances limits and therefore changes what is limiting.

>  Ultimately, the choice is not to grow forever but to decide what limits to live within.

>  Any physical entity with multiple inputs and outputs is surrounded by layers of limits.

>  There always will be limits to growth. They can be self-imposed. If they aren’t, they will be system-imposed.

>  Jay Forrester used to tell us, when we were modeling a construction or processing delay, to ask everyone in the system how long they thought the delay was, make our best guess, and then multiply by three.

>  Changing the length of a delay may utterly change behavior. Delays are often sensitive leverage points for policy, if they can be made shorter or longer.

>  When there are long delays in feedback loops, some sort of foresight is essential. To act only when a problem becomes obvious is to miss an important opportunity to solve the problem.

>  Bounded rationality means that people make quite reasonable decisions based on the information they have. But

>  We don’t even interpret perfectly the imperfect information that we do have, say behavioral scientists.

>  Which is to say, we don’t even make decisions that optimize our own individual good,

>  Economic theory as derived from Adam Smith assumes first that homo economicus acts with perfect optimality on complete information, and second that when many of the species homo economicus do that, their actions add up to the best possible outcome for everybody.

>  God grant us the serenity to exercise our bounded rationality freely in the systems that are structured appropriately, the courage to restructure the systems that aren’t, and the wisdom to know the difference!

>  The bounded rationality of each actor in a system may not lead to decisions that further the welfare of the system as a whole.

>  We call the system structures that produce such common patterns of problematic behavior archetypes. Some of the behaviors these archetypes manifest are addiction, drift to low performance, and escalation.

>  In fact, this system structure can operate in a ratchet mode: Intensification of anyone’s effort leads to intensification of everyone else’s.

>  One way to deal with policy resistance is to try to overpower it. If you wield enough power and can keep wielding it, the power approach can work, at the cost of monumental resentment and the possibility of explosive consequences if the power is ever let up.

>  The alternative to overpowering policy resistance is so counterintuitive that it’s usually unthinkable. Let go. Give up ineffective policies. Let the resources and energy spent on both enforcing and resisting be used for more constructive purposes. You won’t get your way with the system, but it won’t go as far in a bad direction as you think, because much of the action you were trying to correct was in response to your own action.

>  Harmonization of goals in a system is not always possible, but it’s an worth looking for. It can be found only by letting go of more narrow goals and considering the long term welfare of the entire system.

>  In any commons system there is, first of all, a resource that is commonly shared (the pasture). For the system to be subject to tragedy, the resource must be not only limited, but erodable when overused.

>  The tragedy of the commons arises from missing (or too long delayed) feedback from the resource to the growth of the users of that resource.

>  Because there is no feedback to the user, overharvesting will continue. The resource will decline. Finally, the erosion loop will kick in, the resource will be destroyed, and all the users will be ruined.

>  The structure of a commons system makes selfish behavior much more convenient and profitable than behavior that is responsible to the whole community and to the future.

>  There are three ways to avoid the tragedy of the commons.

>  Educate and exhort. Help people to see the consequences of unrestrained use of the commons. Appeal to their morality. Persuade them to be temperate.

>  Privatize the commons. Divide it up, so that each person reaps the consequences of his or her own actions.

>  Regulate the commons. Garrett Hardin calls this option, bluntly, “mutual coercion, mutually agreed upon.” Regulation can take many forms, from outright bans on certain behaviors to quotas, permits, taxes, incentives.

>  When there is a commonly shared resource, every user benefits directly from its use, but shares the costs of its abuse with everyone else.

>  THE WAY OUT Educate and exhort the users, so they understand the consequences of abusing the resource. And also restore or strengthen the missing feedback link, either by privatizing the resource so

>  Another name for this system trap is “eroding goals.” It is also called the “boiled frog syndrome,”

>  There are two antidotes to eroding goals. One is to keep standards absolute, regardless of performance. Another is to make goals sensitive to the best performances of the past, instead of the worst.

>  THE TRAP: DRIFT TO LOW PERFORMANCE Allowing performance standards to be influenced by past performance, especially if there is a negative bias in perceiving past performance, sets up a reinforcing feedback loop of eroding goals that sets a system drifting toward low performance. THE WAY OUT Keep performance standards absolute. Even better, let standards be enhanced by the best actual performances instead of being discouraged by the worst. Use the same structure to set up a drift toward high performance!

>  “I’ll raise you one” is the decision rule that leads to escalation. Escalation comes from a reinforcing loop set up by competing actors trying to get ahead of each other.

>  The goal of one part of the system or one actor is not absolute, like the temperature of a room thermostat being set at 18 ° C (65 ° F), but is related to the state of another part of the system, another actor.

>  Escalation, being a reinforcing feedback loop, builds exponentially. Therefore, it can carry a competition to extremes faster than anyone would believe possible. If nothing is done to break the loop, the process usually ends with one or both of the competitors breaking down.

>  One way out of the escalation trap is unilateral disarmament—deliberately reducing your own system state to induce reductions in your state.

>  The only other graceful way out of the escalation system is to negotiate a disarmament. That’s a structural change, an exercise in system design.

>  “success to the successful.” This system trap is found whenever the winners of a competition receive, as part of the reward, the means to compete even more effectively in the future.

>  Success to the successful is a well-known concept in the field of ecology, where it is called “the competitive exclusion principle.” This principle says that two different species cannot live in exactly the same ecological niche, competing for exactly the same resources. Because the two species are different, one will necessarily reproduce faster, or be able to use the resource more efficiently than the other. It will win a larger share of the resource, which will give it the ability to multiply more and keep winning. It will not only dominate the niche, it will drive the losing competitor to extinction. That will happen not by direct confrontation usually, but by appropriating all the resource, leaving none for the weaker competitor.

>  Species and companies sometimes escape competitive exclusion by diversifying. A species can learn or evolve to exploit new resources. A company can create a new product or service that does not directly compete with existing ones.

>  Diversification is not guaranteed, however, especially if the monopolizing firm (or species) has the power to crush all offshoots, or buy them up, or deprive them of the resources they need to stay alive. Diversification doesn’t work as a strategy for the poor.

>  The most obvious way out of the success-to the-successful archetype is by periodically “leveling the playing field.” Traditional societies and game designers instinctively design into their systems some way of equalizing advantages, so the game stays fair and interesting.

>  If the intervention designed to correct the problem causes the self-maintaining capacity of the original system to atrophy or erode, then a destructive reinforcing feedback loop is set in motion.

>  Systems, like the three wishes in the traditional fairy tale, have a terrible tendency to produce exactly and only what you ask them to produce. Be careful what you ask them to produce. If the desired system state is national security, and that is defined as the amount of money spent on the military, the system will produce military spending. It may or may not produce national security.

>  These examples confuse effort with result, one of the most common mistakes in designing systems around the wrong goal.

>  Specify indicators and goals that reflect the real welfare of the system. Be especially careful not to confuse effort with result or you will end up with a system that is producing effort, not result.

>  Now racing sailboats are extremely fast, highly responsive, and nearly unseaworthy. They need athletic and expert crews to manage them. No one would think of using an America’s Cup yacht for any purpose other than racing within the rules. The boats are so optimized around the present rules that they have lost all resilience. Any change in the rules would render them useless.

>  Leverage points are points of power.

>  But Forrester goes on to point out that although people deeply involved in a system often know intuitively where to find leverage points, more often than not they push the change in the wrong direction.

>  Counterintuitive—that’s Forrester’s word to describe complex systems. Leverage points frequently are not intuitive. Or if they are, we too often use them backward, systematically worsening whatever problems we are trying to solve.

>  But changing these variables rarely changes the behavior of the national economy system. If the system is chronically stagnant, parameter changes rarely kick-start

>  Parameters become leverage points when they go into ranges that kick off one of the items higher on this list. Interest rates, for example, or birth rates, control the gains around reinforcing feedback loops.

>  Consider a huge bathtub with slow in-and outflows. Now think about a small one with very fast flows. That’s the difference between a lake and a river. You hear about catastrophic river floods much more often than catastrophic lake floods, because stocks that are big, relative to their flows, are more stable than small ones.

>  You can often stabilize a system by increasing the capacity of a buffer. 5 But if a buffer is too big, the system gets inflexible. It reacts too slowly. And big buffers of some sorts, such as water reservoirs or inventories, cost a lot to build or maintain.

>  There’s leverage, sometimes magical, in changing the size of buffers. But buffers are usually physical entities, not easy to change.

>  The plumbing structure, the stocks and flows and their physical arrangement, can have an enormous effect on how the system operates.

>  But often physical rebuilding is the slowest and most expensive kind of change to make in a system. Some stock-and-flow structures are just plain unchangeable.

>  Physical structure is crucial in a system, but is rarely a leverage point, because changing it is rarely quick or simple.

>  I would list delay length as a high leverage point, except for the fact that delays are not often easily changeable. Things take as long as they take. You can’t do a lot about the construction time of a major piece of capital, or the maturation time of a child, or the growth rate of a forest. It’s usually easier to slow down the change rate, so that inevitable feedback delays won’t cause so much trouble. That’s why growth rates are higher up on the leverage point list than delay times.

>  But if there is a delay in your system that can be changed, changing it can have big effects. Watch out! Be sure you change it in the right direction!

>  One of the big mistakes we make is to strip away these “emergency” response mechanisms because they aren’t often used and they appear to be costly.

>  The strength of a balancing feedback loop is important relative to the impact it is designed to correct. If the impact increases in strength, the feedbacks have to be strengthened too.

>  A balancing feedback loop is self-correcting; a reinforcing feedback loop is self-reinforcing. The more it works, the more it gains power to work some more, driving system behavior in one direction.

>  Reinforcing feedback loops are sources of growth, explosion, erosion, and collapse in systems. A system with an unchecked reinforcing loop ultimately will destroy itself.

>  Look for leverage points around birth rates, interest rates, erosion rates, “success to the successful” loops, any place where the more you have of something, the more you have the possibility of having more.

>  Missing information flows is one of the most common causes of system malfunction. Adding or restoring information can be a powerful intervention, usually much easier and cheaper than rebuilding physical infrastructure.

>  To take another tragedy of the commons example, it’s not enough to inform all the users of an aquifer that the groundwater level is dropping. That could initiate a race to the bottom. It would be more effective to set the cost of water to rise steeply as the pumping rate begins to exceed the recharge rate.

>  There is a systematic tendency on the part of human beings to avoid accountability for their own decisions. That’s why there are so many missing feedback loops—

>  Power over the rules is real power.

>  If you want to understand the deepest malfunctions of systems, pay attention to the rules and to who has power over them.

>  Self-organization means changing any aspect of a system lower on this list—adding completely new physical structures, such as brains or wings or computers—adding new balancing or reinforcing loops, or new rules. The ability to self-organize is the strongest form of system resilience.

>  As hundreds of self-organizing computer models have demonstrated, complex and delightful patterns can evolve from quite simple sets of rules.

>  Allowing species to go extinct is a systems crime, just as randomly eliminating all copies of particular science journals or particular kinds of scientists would be.

>  Insistence on a single culture shuts down learning and cuts back resilience.

>  The intervention point here is obvious, but unpopular. Encouraging variability and experimentation and diversity means “losing control.” Let a thousand flowers bloom and anything could happen!

>  So how do you change paradigms? Thomas Kuhn, who wrote the seminal book about the great paradigm shifts of science, has a lot to say about that. 8 Clyde Haberman, You keep pointing at the anomalies and failures in the old paradigm. You keep speaking and acting, loudly and with assurance, from the new one. You insert people with the new paradigm in places of public visibility and power. You don’t waste time with reactionaries; rather, you work with active change agents and with the vast middle ground of people who are open-minded.

>  Systems modelers say that we change paradigms by building a model of the system, which takes us outside the system and forces us to see it whole. I say that because my own paradigms have been changed that way.

>  It is to “get” at a gut level the paradigm that there are paradigms, and to see that that itself is a paradigm, and to regard that whole realization as devastatingly funny.

>  A new information feedback loop at this point in this system will make it behave much better. But the decision makers are resistant to the information they need! They don’t pay attention to it, they don’t believe it, they don’t know how to interpret it. Why do people actively sort and screen information the way they do? How do they determine what to let in and what to let bounce off, what to reckon with and what to ignore or disparage?

>  We can’t control systems or figure them out. But we can dance with them!

>  I want to end this chapter and this book by trying to summarize the most general “systems wisdoms” I have absorbed from modeling complex systems and from hanging out with modelers.

>  Before you disturb the system in any way, watch how it behaves.

>  If possible, find or make a time graph of actual data from the system—peoples’ memories are not always reliable when it comes to timing.

>  Starting with the behavior of the system directs one’s thoughts to dynamic, not static, analysis—not only to “What’s wrong?” but also to “How did we get there?” “What other behavior modes are possible?” “If we don’t change direction, where are we going to

>  When we draw structural diagrams and then write equations, we are forced to make our assumptions visible and to express them with rigor. We have to put every one of our assumptions about the system out where others (and we ourselves) can see them.

>  Mental flexibility—the willingness to redraw boundaries, to notice that a system has shifted into a new mode, to see how to redesign structure—is a necessity when you live in a world of flexible systems.

>  would guess that most of what goes wrong in systems goes wrong because of biased, late, or missing information.

>  If I could, I would add an eleventh commandment to the first ten: Thou shalt not distort, delay, or withhold information.

>  Our mental models are mostly verbal. Honoring information means above all avoiding language pollution—making the cleanest possible use we can of language. Second, it means expanding our language so we can talk about complexity.

>  In fact, we don’t talk about what we see; we see only what we can talk about.

>  The first step in respecting language is keeping it as concrete, meaningful, and truthful as possible—part of the job of keeping information streams clear. The second step is to enlarge language to make it consistent with our enlarged understanding of systems.

>  It’s interesting to see what words I had to add when writing this book: feedback, throughput, overshoot, self-organization, sustainability.

>  Our culture, obsessed with numbers, has given us the idea that what we can measure is more important than what we can’t measure. Think about that for a minute. It means that we make quantity more important than quality.

>  Be a quality detector. Be a walking, noisy Geiger counter that registers the presence or absence of quality.

>  Don’t be stopped by the “if you can’t define it and measure it, I don’t have to pay attention to it” ploy.

>  Aim to enhance total systems properties, such as growth, stability, diversity, resilience, and sustainability—whether they are easily measured or not.

>  Before you charge in to make things better, pay attention to the value of what’s already there.

>  “Intrinsic responsibility” means that the system is designed to send feedback about the consequences of decision making directly and quickly and compellingly to the decision makers.

>  One way of making that system more, rather than less, responsible might have been to let professors keep control of their own thermostats and charge them directly for the amount of energy they use, thereby privatizing a commons!

>  These few examples are enough to get you thinking about how little our current culture has come to look for responsibility within the system that generates an action, and how poorly we design systems to experience the consequences of their actions.

